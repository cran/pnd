<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<meta name="author" content="Andreï V. Kostyrka, University of Luxembourg" />


<title>Compatilibility of pnd with the syntax of numDeriv</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>

<style type="text/css">
p.abstract{
text-align: center;
font-weight: bold;
}
div.abstract{
margin: auto;
width: 90%;
}
</style>


<style type="text/css">

div.csl-bib-body { }
div.csl-entry {
clear: both;
margin-bottom: 0em;
}
.hanging div.csl-entry {
margin-left:2em;
text-indent:-2em;
}
div.csl-left-margin {
min-width:2em;
float:left;
}
div.csl-right-inline {
margin-left:2em;
padding-left:1em;
}
div.csl-indent {
margin-left: 2em;
}
</style>

<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">Compatilibility of pnd with the syntax of
numDeriv</h1>
<h4 class="author">Andreï V. Kostyrka, University of Luxembourg</h4>
<h4 class="date">Created: 2024-10-01, last modified: 2025-03-08,
compiled: 2025-09-03</h4>
<div class="abstract">
<p class="abstract">Abstract</p>
We describe the syntax of the widely popular <code>numDeriv</code>
package and show how the functions from the <code>pnd</code> package
recognise and handle the parameters related to numerical difference
computation. We draw parallels between the two, examine the differences,
and provide recommendations for new users.
</div>


<div id="TOC">
<ul>
<li><a href="#definitions-related-to-numerical-derivatives" id="toc-definitions-related-to-numerical-derivatives"><span class="toc-section-number">1</span> Definitions related to numerical
derivatives</a>
<ul>
<li><a href="#quick-start-reproducing-the-numderiv-vignette" id="toc-quick-start-reproducing-the-numderiv-vignette"><span class="toc-section-number">1.1</span> Quick start: reproducing the
numDeriv vignette</a></li>
<li><a href="#vectorisation-pitfalls" id="toc-vectorisation-pitfalls"><span class="toc-section-number">1.2</span> Vectorisation pitfalls</a></li>
</ul></li>
<li><a href="#breakdown-of-numderivgrad" id="toc-breakdown-of-numderivgrad"><span class="toc-section-number">2</span> Breakdown of
<code>numDeriv::grad</code></a>
<ul>
<li><a href="#handling-vectorised-inputs" id="toc-handling-vectorised-inputs"><span class="toc-section-number">2.1</span> Handling vectorised
inputs</a></li>
<li><a href="#approximation-method" id="toc-approximation-method"><span class="toc-section-number">2.2</span> Approximation method</a></li>
</ul></li>
<li><a href="#compatibility-implies-syntax-support-not-identical-values" id="toc-compatibility-implies-syntax-support-not-identical-values"><span class="toc-section-number">3</span> Compatibility implies syntax
support, not identical values</a>
<ul>
<li><a href="#numderiv-zero-handling-has-a-discontinuity" id="toc-numderiv-zero-handling-has-a-discontinuity"><span class="toc-section-number">3.1</span> numDeriv zero handling has a
discontinuity</a></li>
<li><a href="#not-all-evaluations-matter-for-richardson-extrapolation" id="toc-not-all-evaluations-matter-for-richardson-extrapolation"><span class="toc-section-number">3.2</span> Not all evaluations matter for
Richardson extrapolation</a></li>
</ul></li>
<li><a href="#diagnostics" id="toc-diagnostics"><span class="toc-section-number">4</span> Diagnostics</a>
<ul>
<li><a href="#higher-order-accuracy-diagnostics" id="toc-higher-order-accuracy-diagnostics"><span class="toc-section-number">4.1</span> Higher-order accuracy
diagnostics</a></li>
</ul></li>
<li><a href="#references" id="toc-references"><span class="toc-section-number">5</span> References</a></li>
</ul>
</div>

<p>Load the packages first:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(pnd)</span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a><span class="co">#&gt; Parallel numerical derivatives v.0.1.1 (2025-09-04).</span></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="co">#&gt; 24 physical cores for parallelism through mclapply forking are available on Linux.</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a><span class="fu">library</span>(numDeriv)</span></code></pre></div>
<p>Numerical derivatives are ubiquitous in applies statistics and
numerical methods, which is why many packages rely on streamlined
implementations of numerical derivatives as dependencies. The popular
<code>numDeriv</code> package by Paul Gilbert and Ravi Varadhan has 314
reverse dependencies as of October 2024. Its flexibility allowed other
packages to achieve better results by providing reasonable defaults and
the interface for fine-tuning that matters in numerical optimisation and
statistical inference. However, it has no parallel capabilities, which
is why it takes a lot of time to calculate numerical gradients with its
<code>grad</code> function.</p>
<p>This vignette showcases the similarities and differences between the
features of <code>numDeriv</code> and <code>pnd</code> for a smooth and
painless transition to the new package.</p>
<div id="definitions-related-to-numerical-derivatives" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Definitions related to
numerical derivatives</h1>
<p>In this section, we overview the key definitions that relate to the
functionality of <code>numDeriv</code>.</p>
<p>A <strong>derivative</strong> is the instantaneous rate of change of
a function (assuming its differentiability): <span class="math display">\[
f&#39;(x) = \frac{\mathrm{d} f}{\mathrm{d} x} := \lim_{h\to0}
\frac{f(x+h) - f(x)}{h}
\]</span></p>
<p>A <strong>numerical derivative</strong> is an approximation of the
expression above for a small fixed <em>h</em>. For a scalar function,
this approximation can be calculated with two function evaluations at
two different points yielding forward, central, and backward numerical
differences respectively: <span class="math display">\[
f_{\mathrm{FD}}&#39; (x, h) := \frac{f(x+h) - f(x)}{h}, \quad
f_{\mathrm{CD}}&#39; (x, h) := \frac{f(x+h) - f(x-h)}{2h}, \quad
f_{\mathrm{BD}}&#39; (x, h) := \frac{f(x) - f(x-h)}{h}
\]</span></p>
<p>These expressions involve only 2 terms, which limits their accuracy.
Ignoring machine errors due to rounding, the approximation error of
<span class="math inline">\(f_{\mathrm{FD}}&#39; (x, h)\)</span> and
<span class="math inline">\(f_{\mathrm{BD}}&#39; (x, h)\)</span> due to
Taylor series truncation is of the order <span class="math inline">\(O(h)\)</span>, and for <span class="math inline">\(f_{\mathrm{CD}}&#39; (x, h)\)</span>, it is <span class="math inline">\(O(h^2)\)</span>; since in practice, <span class="math inline">\(h\)</span> is very small, <span class="math inline">\(f_{\mathrm{CD}}&#39; (x, h)\)</span> is more
accurate. The degree to which <span class="math inline">\(h\)</span> is
raised in the error term is reflected in the naming: <span class="math inline">\(f_{\mathrm{FD}}&#39; (x, h)\)</span> and <span class="math inline">\(f_{\mathrm{BD}}&#39; (x, h)\)</span> are
<em>first-order-accurate</em>, while <span class="math inline">\(f_{\mathrm{CD}}&#39; (x, h)\)</span> is
<em>second-order-accurate</em>. Higher accuracy can be achieved by
computing more terms at extra points; the following approximation is
<em>fourth-order-accurate</em>: <span class="math display">\[
f_{\mathrm{CD},4}&#39; (x, h) := \frac{f(x - 2h) -8 f(x-h) + 8 f(x+h) -
f(x+2h)}{12h}
\]</span> Instead of <span class="math inline">\((x\pm h, x \pm
2h)\)</span>, any other convenient grid may be chosen. For the sake of
brevity, we do not compare the accuracy of these approximations and
refer to another vignette of this package, ‘Step-size-selection
algorithm benchmark’.</p>
<p>Instead of choosing a large evaluation grid in advance, the
researcher may instead compute the numerical derivative for several
different step sizes, <span class="math inline">\(h_1 &gt; h_2 &gt;
\ldots\)</span>, and combine multiple approximation to achieve higher
accuracy. This is known as <strong>Richardson extrapolation</strong>:
<span class="math display">\[
f_{\mathrm{Rich},4}&#39; (x, h_1, h_2) := \frac{(h_1 / h_2)^2
f_{\mathrm{CD}}&#39; (x, h_2) - f_{\mathrm{CD}}&#39; (x, h_1)}{(h_1 /
h_2)^2 - 1}
\]</span></p>
<p>If <span class="math inline">\(h_2 = h_1 / 2\)</span>, then, this
extrapolation formula reduces to <span class="math inline">\(f_{\mathrm{CD},4}&#39; (x, h_2)\)</span>. There is
a general solution to obtaining high-order-accurate derivatives from
function values evaluated at multiple points; see <span class="citation">Fornberg (1988)</span> for the solution and the
algorithm and <span class="citation">Kostyrka (2025)</span> for
derivation and stability analysis.</p>
<p><strong>NB.</strong> For accuracy order <span class="math inline">\(a
&gt; 2\)</span>, more than 2 function evaluations are required, which is
why the notion of ‘step size’ becomes unclear: it could signify the
initial step size in the Richardson extrapolation before the first
reduction, or the space between the elements of a symmetric grid <span class="math inline">\(\{\pm h, \pm 2h, \pm 3h, \ldots\}\)</span>
excluding zero, or the space between the elements of an equispaced
symmetric grid <span class="math inline">\(\{\pm h, \pm 3h, \pm 5h,
\ldots\}\)</span>, or some other measure describing the scaling of the
deviations around the point of interest.</p>
<p>Therefore, to estimate a derivative numerically, the user should be
able to supply the following tweaking parameters:</p>
<ul>
<li>The function <span class="math inline">\(f\)</span> and evaluation
point <span class="math inline">\(x\)</span>;</li>
<li>The side to determine whether forward, central, or backward
differences should be calculated;</li>
<li>The step size <span class="math inline">\(h\)</span>;</li>
<li>The desired accuracy order <span class="math inline">\(a\)</span>
for the truncation error to be <span class="math inline">\(O(h^a)\)</span> (requiring at least <span class="math inline">\(2a\)</span> evaluations). (Assuming, as described
in the note above, that it means either the initial step size or the
grid spacing based on the implementation.)</li>
</ul>
<p>Vector cases and, therefore, Jacobians can be approximated by
applying these formulæ to each coordinate of <span class="math inline">\(f\)</span>: <span class="math display">\[
J_{f_4}(x) :=
\begin{bmatrix}
    \frac{\partial f^{(1)}_4}{\partial x^{(1)}} &amp; \cdots &amp;
\frac{\partial f^{(1)}_4}{\partial x^{(n)}} \\
    \vdots                             &amp; \ddots &amp; \vdots \\
    \frac{\partial f^{(m)}_4}{\partial x^{(1)}} &amp; \cdots &amp;
\frac{\partial f^{(m)}_4}{\partial x^{(n)}}
\end{bmatrix}(x),
\]</span> where each partial derivative is estimated via finite
differences.</p>
<p>Both <code>numDeriv</code> and <code>pnd</code> allow the user to
choose these parameters. The simplified comparison is given below.</p>
<table>
<colgroup>
<col width="19%" />
<col width="51%" />
<col width="29%" />
</colgroup>
<thead>
<tr class="header">
<th>Argument</th>
<th><code>numDeriv::grad()</code> syntax</th>
<th><code>pnd::Grad()</code> syntax</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Function <span class="math inline">\(f\)</span></td>
<td><code>func</code></td>
<td><code>FUN</code></td>
</tr>
<tr class="even">
<td>Point <span class="math inline">\(x\)</span></td>
<td><code>x</code></td>
<td><code>x</code></td>
</tr>
<tr class="odd">
<td>Side</td>
<td><code>side</code></td>
<td><code>side</code></td>
</tr>
<tr class="even">
<td>Step size</td>
<td><code>method.args = list(d = ..., eps = ...)</code></td>
<td><code>h = ...</code></td>
</tr>
<tr class="odd">
<td>Accuracy order</td>
<td><code>method.args = list(r = ...)</code></td>
<td><code>acc.order = ...</code></td>
</tr>
</tbody>
</table>
<div id="quick-start-reproducing-the-numderiv-vignette" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Quick start:
reproducing the numDeriv vignette</h2>
<p>Examples of derivative and gradient calculation:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="fu">grad</span>(sin, pi)  <span class="co"># Old</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a><span class="co">#&gt; [1] -1</span></span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a><span class="fu">Grad</span>(sin, pi)  <span class="co"># New</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a><span class="co">#&gt; Estimated derivative: -1.0000</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="co">#&gt; (default step size: 1.9e-05).</span></span></code></pre></div>
<p>Vector arguments are supported:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a><span class="fu">grad</span>(sin, (<span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>pi<span class="sc">/</span><span class="dv">10</span>)  <span class="co"># Old</span></span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a><span class="co">#&gt;  [1]  1.000000  0.809017  0.309017 -0.309017 -0.809017 -1.000000 -0.809017</span></span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a><span class="co">#&gt;  [8] -0.309017  0.309017  0.809017  1.000000</span></span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a><span class="fu">Grad</span>(sin, (<span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>pi<span class="sc">/</span><span class="dv">10</span>)  <span class="co"># New</span></span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a><span class="co">#&gt; Estimated gradient:</span></span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a><span class="co">#&gt; 1.0000  0.8090  0.3090  -0.3090  -0.8090  -1.0000  -0.8090  -0.3090  0.3090  0.8090  1.0000</span></span>
<span id="cb3-7"><a href="#cb3-7" tabindex="-1"></a><span class="co">#&gt; (default step size: 1.5e-08, 5.2e-06, 7.6e-06, 1.1e-05, 1.5e-05, 1.9e-05, 2.3e-05, 2.7e-05, 3.0e-05, 3.4e-05, 3.8e-05).</span></span></code></pre></div>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a>func0 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">sum</span>(<span class="fu">sin</span>(x))</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a><span class="fu">grad</span>(func0, (<span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>pi<span class="sc">/</span><span class="dv">10</span>)  <span class="co"># Old</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a><span class="co">#&gt;  [1]  1.000000  0.809017  0.309017 -0.309017 -0.809017 -1.000000 -0.809017</span></span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a><span class="co">#&gt;  [8] -0.309017  0.309017  0.809017  1.000000</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a><span class="fu">Grad</span>(func0, (<span class="dv">0</span><span class="sc">:</span><span class="dv">10</span>)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>pi<span class="sc">/</span><span class="dv">10</span>)  <span class="co"># New</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="co">#&gt; Estimated gradient:</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="co">#&gt; 1.0000  0.8090  0.3090  -0.3090  -0.8090  -1.0000  -0.8090  -0.3090  0.3090  0.8090  1.0000</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a><span class="co">#&gt; (default step size: 1.5e-08, 5.2e-06, 7.6e-06, 1.1e-05, 1.5e-05, 1.9e-05, 2.3e-05, 2.7e-05, 3.0e-05, 3.4e-05, 3.8e-05).</span></span></code></pre></div>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a>func1 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">sin</span>(<span class="dv">10</span><span class="sc">*</span>x) <span class="sc">-</span> <span class="fu">exp</span>(<span class="sc">-</span>x)</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a><span class="fu">curve</span>(func1, <span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">5</span>)</span></code></pre></div>
<p><img role="img" src="data:image/png;base64,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" width="10cm" /></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fl">2.04</span></span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>numd1 <span class="ot">&lt;-</span> <span class="fu">grad</span>(func1, x)  <span class="co"># Old</span></span>
<span id="cb6-3"><a href="#cb6-3" tabindex="-1"></a>numd2 <span class="ot">&lt;-</span> <span class="fu">Grad</span>(func1, x)  <span class="co"># New</span></span>
<span id="cb6-4"><a href="#cb6-4" tabindex="-1"></a>numd3 <span class="ot">&lt;-</span> <span class="fu">Grad</span>(func1, x, <span class="at">h =</span> <span class="fu">gradstep</span>(func1, x)<span class="sc">$</span>par)  <span class="co"># New auto-selection</span></span>
<span id="cb6-5"><a href="#cb6-5" tabindex="-1"></a>exact <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">*</span><span class="fu">cos</span>(<span class="dv">10</span><span class="sc">*</span>x) <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>x)</span>
<span id="cb6-6"><a href="#cb6-6" tabindex="-1"></a><span class="fu">c</span>(<span class="at">Exact =</span> exact, <span class="at">Old =</span> numd1, <span class="at">New =</span> numd2, <span class="at">NewAuto =</span> numd2,</span>
<span id="cb6-7"><a href="#cb6-7" tabindex="-1"></a>  <span class="at">OldErr =</span> (numd1<span class="sc">-</span>exact)<span class="sc">/</span>exact, <span class="at">NewErr =</span> (numd2<span class="sc">-</span>exact)<span class="sc">/</span>exact,</span>
<span id="cb6-8"><a href="#cb6-8" tabindex="-1"></a>  <span class="at">NewAutoErr =</span> (numd3<span class="sc">-</span>exact)<span class="sc">/</span>exact)</span>
<span id="cb6-9"><a href="#cb6-9" tabindex="-1"></a><span class="co">#&gt;      Exact        Old        New    NewAuto     OldErr     NewErr NewAutoErr </span></span>
<span id="cb6-10"><a href="#cb6-10" tabindex="-1"></a><span class="co">#&gt;  3.335e-01  3.335e-01  3.335e-01  3.335e-01  8.469e-12 -1.530e-09  2.977e-09</span></span></code></pre></div>
<p>Here, the relative error of the new approach with default settings is
worse than that of the old one. Nevertheless, the new implementation is
much faster and attains satisfactory accuracy with only 2 function
calls, whilst the <code>numDeriv</code> counterpart calls the function 9
times. If an appropriate step size is chosen (e.g. with the showcased
auto-selection procedure), then, the reliability of the result increases
while the derivative calculation still requires only 2 evaluations.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>)</span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a>numd1 <span class="ot">&lt;-</span> <span class="fu">grad</span>(func1, x)  <span class="co"># Old</span></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a>numd2 <span class="ot">&lt;-</span> <span class="fu">Grad</span>(func1, x)  <span class="co"># New</span></span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a>numd3 <span class="ot">&lt;-</span> <span class="fu">Grad</span>(func1, x, <span class="at">h =</span> <span class="fu">sapply</span>(x, <span class="cf">function</span>(y) <span class="fu">gradstep</span>(func1, y)<span class="sc">$</span>par))</span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>exact <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">*</span><span class="fu">cos</span>(<span class="dv">10</span><span class="sc">*</span>x) <span class="sc">+</span> <span class="fu">exp</span>(<span class="sc">-</span>x)</span>
<span id="cb7-6"><a href="#cb7-6" tabindex="-1"></a><span class="fu">cbind</span>(<span class="at">Exact =</span> exact, <span class="at">Old =</span> numd1, <span class="at">New =</span> numd2, <span class="at">NewAuto =</span> numd2,</span>
<span id="cb7-7"><a href="#cb7-7" tabindex="-1"></a> <span class="at">OldErr =</span> (numd1<span class="sc">-</span>exact)<span class="sc">/</span>exact, <span class="at">NewErr =</span> (numd2<span class="sc">-</span>exact)<span class="sc">/</span>exact,</span>
<span id="cb7-8"><a href="#cb7-8" tabindex="-1"></a>  <span class="at">NewAutoErr =</span> (numd3<span class="sc">-</span>exact)<span class="sc">/</span>exact)</span>
<span id="cb7-9"><a href="#cb7-9" tabindex="-1"></a><span class="co">#&gt;        Exact    Old    New NewAuto     OldErr     NewErr NewAutoErr</span></span>
<span id="cb7-10"><a href="#cb7-10" tabindex="-1"></a><span class="co">#&gt;  [1,] -8.023 -8.023 -8.023  -8.023  6.118e-12 -6.443e-10  1.965e-09</span></span>
<span id="cb7-11"><a href="#cb7-11" tabindex="-1"></a><span class="co">#&gt;  [2,]  4.216  4.216  4.216   4.216  6.271e-12 -2.370e-09  2.843e-09</span></span>
<span id="cb7-12"><a href="#cb7-12" tabindex="-1"></a><span class="co">#&gt;  [3,]  1.592  1.592  1.592   1.592 -3.507e-12 -5.323e-09  2.863e-09</span></span>
<span id="cb7-13"><a href="#cb7-13" tabindex="-1"></a><span class="co">#&gt;  [4,] -6.651 -6.651 -6.651  -6.651  6.698e-12 -9.812e-09  1.502e-08</span></span>
<span id="cb7-14"><a href="#cb7-14" tabindex="-1"></a><span class="co">#&gt;  [5,]  9.656  9.656  9.656   9.656 -7.892e-13 -1.527e-08  1.344e-08</span></span>
<span id="cb7-15"><a href="#cb7-15" tabindex="-1"></a><span class="co">#&gt;  [6,] -9.522 -9.522 -9.522  -9.522 -4.610e-12 -2.200e-08 -1.448e-08</span></span>
<span id="cb7-16"><a href="#cb7-16" tabindex="-1"></a><span class="co">#&gt;  [7,]  6.334  6.334  6.334   6.334  1.243e-11 -2.993e-08  9.228e-09</span></span>
<span id="cb7-17"><a href="#cb7-17" tabindex="-1"></a><span class="co">#&gt;  [8,] -1.104 -1.104 -1.104  -1.104  6.093e-12 -3.913e-08  1.172e-08</span></span>
<span id="cb7-18"><a href="#cb7-18" tabindex="-1"></a><span class="co">#&gt;  [9,] -4.481 -4.481 -4.481  -4.481  1.524e-12 -4.952e-08  7.520e-09</span></span>
<span id="cb7-19"><a href="#cb7-19" tabindex="-1"></a><span class="co">#&gt; [10,]  8.623  8.623  8.623   8.623 -8.120e-13 -6.111e-08 -2.153e-08</span></span></code></pre></div>
<p>Examples of Jacobian calculation:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>func2 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">c</span>(<span class="fu">sin</span>(x), <span class="fu">cos</span>(x))</span>
<span id="cb8-2"><a href="#cb8-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> (<span class="dv">0</span><span class="sc">:</span><span class="dv">1</span>)<span class="sc">*</span><span class="dv">2</span><span class="sc">*</span>pi</span>
<span id="cb8-3"><a href="#cb8-3" tabindex="-1"></a><span class="fu">jacobian</span>(func2, x)</span>
<span id="cb8-4"><a href="#cb8-4" tabindex="-1"></a><span class="co">#&gt;      [,1] [,2]</span></span>
<span id="cb8-5"><a href="#cb8-5" tabindex="-1"></a><span class="co">#&gt; [1,]    1    0</span></span>
<span id="cb8-6"><a href="#cb8-6" tabindex="-1"></a><span class="co">#&gt; [2,]    0    1</span></span>
<span id="cb8-7"><a href="#cb8-7" tabindex="-1"></a><span class="co">#&gt; [3,]    0    0</span></span>
<span id="cb8-8"><a href="#cb8-8" tabindex="-1"></a><span class="co">#&gt; [4,]    0    0</span></span>
<span id="cb8-9"><a href="#cb8-9" tabindex="-1"></a><span class="fu">Jacobian</span>(func2, x)</span>
<span id="cb8-10"><a href="#cb8-10" tabindex="-1"></a><span class="co">#&gt; Estimated Jacobian:</span></span>
<span id="cb8-11"><a href="#cb8-11" tabindex="-1"></a><span class="co">#&gt; 1.0000  1.0000</span></span>
<span id="cb8-12"><a href="#cb8-12" tabindex="-1"></a><span class="co">#&gt; 0       0     </span></span>
<span id="cb8-13"><a href="#cb8-13" tabindex="-1"></a><span class="co">#&gt; (default step size range: 1.5e-08...3.8e-05.)</span></span></code></pre></div>
<p>Examples of Hessian calculation:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fl">0.25</span> <span class="sc">*</span> pi</span>
<span id="cb9-2"><a href="#cb9-2" tabindex="-1"></a><span class="fu">hessian</span>(sin, x)</span>
<span id="cb9-3"><a href="#cb9-3" tabindex="-1"></a><span class="co">#&gt;            [,1]</span></span>
<span id="cb9-4"><a href="#cb9-4" tabindex="-1"></a><span class="co">#&gt; [1,] -0.7071068</span></span>
<span id="cb9-5"><a href="#cb9-5" tabindex="-1"></a>fun1e <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">sum</span>(<span class="fu">exp</span>(<span class="dv">2</span><span class="sc">*</span>x))</span>
<span id="cb9-6"><a href="#cb9-6" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>)</span>
<span id="cb9-7"><a href="#cb9-7" tabindex="-1"></a><span class="fu">hessian</span>(fun1e, x, <span class="at">method.args=</span><span class="fu">list</span>(<span class="at">d=</span><span class="fl">0.01</span>))</span>
<span id="cb9-8"><a href="#cb9-8" tabindex="-1"></a><span class="co">#&gt;              [,1]         [,2]         [,3]</span></span>
<span id="cb9-9"><a href="#cb9-9" tabindex="-1"></a><span class="co">#&gt; [1,] 2.955622e+01 2.583236e-15 1.620102e-11</span></span>
<span id="cb9-10"><a href="#cb9-10" tabindex="-1"></a><span class="co">#&gt; [2,] 2.583236e-15 1.613715e+03 5.400691e-12</span></span>
<span id="cb9-11"><a href="#cb9-11" tabindex="-1"></a><span class="co">#&gt; [3,] 1.620102e-11 5.400691e-12 8.810586e+04</span></span></code></pre></div>
</div>
<div id="vectorisation-pitfalls" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Vectorisation
pitfalls</h2>
<p>Vectorisation does not occur naturally in computer science; it is
rather a feature of a specially designed system than a natural property
of hardware or software. From the theoretical point of view, computers
are imperfect, restricted versions of Turing machine, analogous to
deterministic finite automata or linear bounded automata, capable of
reading and writing on a ‘tape’ of practically limited size provided a
sequence of instructions. At the hardware level, computers are capable
of executing a set of processor instructions, and these instructions are
typically supplied sequentially. In fact, vectorisation at the hardware
level is a relatively rare phenomenon because without a proper set-up,
it may lead to race conditions and non-deterministic behaviour. In
certain architectures or extensions (MMX, SSE, AVX), the data for
calculations can be set up in such a manner that one single instruction
processes multiple inputs, which can be achieved by packing the values
into one large register (e.g. four 32-bit numbers into one 128-bit SIMD
register). However, it is not easy to organise control flow in such a
manner, which is why most computer operations are carried out
sequentially.</p>
<p>The strong point of the <strong>R</strong> language is its
vectorisation. One may argue that this feature leads to its inefficiency
compared to, e.g., C++. Indeed, in C++, the overhead in loops is
typically irreducible and boils down to such hard-to-control factors as
jump instructions and data alignment to memory pages, whereas in R, the
overhead appear at each loop iteration due to environment manipulation,
extra assignment, name look-up, and occasional class-specific method
execution. On the other hand, not writing loops is useful when it would
take more human time to allocate memory and run a loop than computer
time to repeat redundant high-level operations under the hood: what
matters is the total time saved, machine and human combined. Finally, if
there is no explicit loop in a function because the latter relies
internally on <code>apply()</code> or calls C/C++ loops on high-level or
computationally heavy objects, then, the overhead due to the high-level
nature of R does not create substantial time losses compared to the
inner computations, and a vectorised function can be as slow as its
loop-based sequential version. This can be seen in the definition of
<code>do_vapply</code> in <code>apply.c</code>: the user-supplied
function is called in a loop; the implementation of
<code>SEXP lapply</code> also contains a C loop. Therefore, in an
efficient implementation of a vectorised function, the overhead is
minimised if most of the loops in which the input elements are processed
are low-level C/C++ loops with as few exchanges and high-level syntactic
embellishments as possible.</p>
<p>In <strong>R</strong>, it is harder to find something that is
<em>not</em> a vector. A scalar is just a special case of a vector –
with length 1: <code>identical(c(1), 1)</code> is <code>TRUE</code>.
Therefore, if a function is implemented in such a manner that it can
produce vector outputs for vector inputs <em>en masse</em>, it is
considered to be vectorised – with a caveat: dimensionality matters.
Consider the following classification:</p>
<table>
<colgroup>
<col width="31%" />
<col width="34%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Scalar output</th>
<th>Vector output</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Scalar input</td>
<td><span class="math inline">\(f_1\colon \mathbb{R} \mapsto
\mathbb{R}\)</span></td>
<td><span class="math inline">\(f_2\colon \mathbb{R} \mapsto
\mathbb{R}^m\)</span></td>
</tr>
<tr class="even">
<td>Vector input</td>
<td><span class="math inline">\(f_3\colon \mathbb{R}^n \mapsto
\mathbb{R}\)</span></td>
<td><span class="math inline">\(f_4\colon \mathbb{R}^n \mapsto
\mathbb{R}^m\)</span></td>
</tr>
</tbody>
</table>
<p>To compute a numerical derivative via finite differences, the
function must be evaluated at least twice. This means that, depending on
the parallel capabilities of a function, an efficient programmer should
implement numerical differentiation in such a manner that functions that
are vectorised at the low level take as many arguments in one call as
possible. Many calls of the same function with shorter arguments are
slower than one call with a long argument:</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="fu">system.time</span>(<span class="fu">replicate</span>(<span class="fl">1e5</span>, <span class="fu">sin</span>(<span class="fu">rnorm</span>(<span class="dv">10</span>))))</span>
<span id="cb10-2"><a href="#cb10-2" tabindex="-1"></a><span class="co">#&gt;    user  system elapsed </span></span>
<span id="cb10-3"><a href="#cb10-3" tabindex="-1"></a><span class="co">#&gt;   0.274   0.008   0.284</span></span>
<span id="cb10-4"><a href="#cb10-4" tabindex="-1"></a><span class="fu">system.time</span>(<span class="fu">sin</span>(<span class="fu">rnorm</span>(<span class="fl">1e6</span>)))</span>
<span id="cb10-5"><a href="#cb10-5" tabindex="-1"></a><span class="co">#&gt;    user  system elapsed </span></span>
<span id="cb10-6"><a href="#cb10-6" tabindex="-1"></a><span class="co">#&gt;   0.071   0.000   0.071</span></span></code></pre></div>
<p>The same phenomenon is observed in the real world with storage media:
copying 1000 files of size 1 MB each onto a flash drive is slower than
copying one 1-GB file obtained by concatenating said 1000 files.</p>
<p>The case of non-vectorised scalar output for scalar input, <span class="math inline">\(f_1\)</span>, can be vectorised trivially by
putting the function inside a loop or an <code>apply</code>-like
operation. Then, the preparation task for approximating <span class="math inline">\(f&#39;_1\)</span> is creating an array of values
at which the function will be evaluated in a loop or in a sequence of
parallel jobs (if <span class="math inline">\(f_1\)</span> is
computationally costly and the parallelisation overhead is small
compared to the evaluation time of <span class="math inline">\(f_1\)</span>). This is the case if <span class="math inline">\(f_1(x_0)\)</span> has length 1 for scalar <span class="math inline">\(x_0\)</span> and throws and error for vector <span class="math inline">\(x_0\)</span>.</p>
<p>If the function <span class="math inline">\(f_2\)</span> returns a
vector of length <span class="math inline">\(m\)</span> for scalar
input, then, its coordinate-wise derivatives make up a Jacobian with
dimensions <span class="math inline">\(m\times 1\)</span>. Such a
function should return an error for vector input, but a list of scalar
inputs should produce a list of vector outputs in a loop / apply-like
call that can be combined to obtain the <strong>Jacobian</strong>
approximation. The user should be aware of this function behaviour and
specifically request a Jacobian.</p>
<p>The case of <span class="math inline">\(f_3\)</span> is simple: if
the output has length 1 for vector input of length <span class="math inline">\(n\)</span>, then it implies that <span class="math inline">\(f_3\)</span> carries out a dimensionality-reducing
operation on the input elements. This implies that the numerical
derivatives with respect to each coordinate form the
<strong>gradient</strong> vector, and the user should call the function
that computes the gradient approximation. The numerical derivative with
respect to the first coordinate of <span class="math inline">\(x_0\)</span> is the weighted sum of <span class="math inline">\(f(x_0^{(1)} + h, x_0^{(2)}, \ldots)\)</span>,
<span class="math inline">\(f(x_0^{(1)} - h, x_0^{(2)},
\ldots)\)</span>, and potentially other values obtained at points where
only the first coordinate of <span class="math inline">\(x_0\)</span> is
altered. This is repeated for each input coordinate: e.g., for central
differences, <span class="math inline">\(2n\)</span> evaluations are
expected. The input is therefore a list of <span class="math inline">\(2n\)</span> argument vectors of length <span class="math inline">\(n\)</span>, and potentially more elements for
higher accuracy or higher derivation order.</p>
<p>The case with <span class="math inline">\(f_4\)</span> is the most
complicated one, and it is the reason behind the unexpected behaviour of
some of the functions in <code>numDeriv</code>. There are two sources of
danger in this implementation.</p>
<p><strong>1.</strong> If <span class="math inline">\(n = m\)</span>,
i.e. the user supplies a vector of length <span class="math inline">\(n\)</span> and the function returns a vector of
length <span class="math inline">\(n\)</span>, it is either due to the
true vectorisation of <span class="math inline">\(f_4\)</span> or due to
the fact that the output always has length <span class="math inline">\(n\)</span> regardless of the input, and such a
coincidence happened that the input had length <span class="math inline">\(n\)</span>. It is impossible to claim that <span class="math inline">\(f_4\colon \mathbb{R}^n \mapsto
\mathbb{R}^n\)</span> is a vectorised <span class="math inline">\(\vec
f_1\colon \mathbb{R} \mapsto \mathbb{R}\)</span> solely from the fact
that the output has length <span class="math inline">\(n\)</span> for
some input of length <span class="math inline">\(n\)</span>; multiple
lengths should be checked to confirm this.</p>
<p><strong>Example 1.</strong> Consider a function that computes the
quartiles of its input series. Its output always has length 3.
<code>numDeriv::grad</code> will believe that this function is a
vectorised scalar-valued one (i.e. <span class="math inline">\(\vec
f_1\)</span>) if <code>length(x) == 3</code>:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">quantile</span>(x, <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span><span class="sc">/</span><span class="dv">4</span>)</span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a><span class="fu">grad</span>(f, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>)</span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a><span class="co">#&gt; Error in grad.default(f, x = 1:2): grad assumes a scalar valued function.</span></span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a><span class="fu">grad</span>(f, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a><span class="co">#&gt; Error in grad.default(f, x = 1:4): grad assumes a scalar valued function.</span></span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a><span class="fu">grad</span>(f, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>)</span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a><span class="co">#&gt; [1] 1.5000000 1.0000000 0.8333333</span></span></code></pre></div>
<p>It is therefore necessary to explicitly call its Jacobian:</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">jacobian</span>(f, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>)</span>
<span id="cb12-2"><a href="#cb12-2" tabindex="-1"></a><span class="co">#&gt;      [,1] [,2] [,3]</span></span>
<span id="cb12-3"><a href="#cb12-3" tabindex="-1"></a><span class="co">#&gt; [1,]  0.5  0.5  0.0</span></span>
<span id="cb12-4"><a href="#cb12-4" tabindex="-1"></a><span class="co">#&gt; [2,]  0.0  1.0  0.0</span></span>
<span id="cb12-5"><a href="#cb12-5" tabindex="-1"></a><span class="co">#&gt; [3,]  0.0  0.5  0.5</span></span>
<span id="cb12-6"><a href="#cb12-6" tabindex="-1"></a><span class="fu">jacobian</span>(f, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb12-7"><a href="#cb12-7" tabindex="-1"></a><span class="co">#&gt;      [,1] [,2] [,3] [,4]</span></span>
<span id="cb12-8"><a href="#cb12-8" tabindex="-1"></a><span class="co">#&gt; [1,] 0.25 0.75 0.00 0.00</span></span>
<span id="cb12-9"><a href="#cb12-9" tabindex="-1"></a><span class="co">#&gt; [2,] 0.00 0.50 0.50 0.00</span></span>
<span id="cb12-10"><a href="#cb12-10" tabindex="-1"></a><span class="co">#&gt; [3,] 0.00 0.00 0.75 0.25</span></span></code></pre></div>
<p>This occurrence is more likely if <span class="math inline">\(n\)</span> and <span class="math inline">\(m\)</span> are small: for long inputs, a long
output vector of identical length implies almost certainly that <span class="math inline">\(f_4\)</span> is a vectorised <span class="math inline">\(\vec f_1\colon \mathbb{R} \mapsto
\mathbb{R}\)</span>, enabling efficient evaluation grid preparation to
keep the overhead due to loops as low as possible. <span class="math inline">\(\blacksquare\)</span></p>
<p><strong>2.</strong> If <span class="math inline">\(n\ne m\)</span>
but the function is internally parallelised coordinate-wise, then,
dimension checks based on the observed input and output lengths may be
flawed and result in unexpected behaviour. Technically, computing <span class="math inline">\(f_4\colon \mathbb{R}^n \mapsto
\mathbb{R}^m\)</span> can be seen as repeated application of <span class="math inline">\(f_2\colon \mathbb{R} \mapsto \mathbb{R}^m\)</span>
to a list of <span class="math inline">\(n\)</span> points, and its
parallelisation can be envisioned as such. However, if <span class="math inline">\(f_4\)</span> is implemented as stacked <span class="math inline">\(f_3\colon \mathbb{R}^n \mapsto
\mathbb{R}\)</span>, the output should be trated differently.</p>
<p>The Jacobian matrix can be created by column, stacking evaluations of
<span class="math inline">\(\frac{\partial}{\partial x}
f^{(i)}_4(x)\)</span> at each point <span class="math inline">\(x_1,
\ldots, x_n\)</span>, or by row, binding the transposed gradients <span class="math inline">\(\nabla^\intercal f^{(i)}_4(x)\)</span> of each
coordinate of <span class="math inline">\(f\)</span>. In
<strong>R</strong>, matrices can be created by column (default) or by
row, which determines the order in which the vector of length <span class="math inline">\(nm\)</span> is wrapped, but the exact method has
to be decided by the user or, in case of a package, inferred from the
function behaviour.</p>
<p><strong>Example 2.</strong> Consider <span class="math inline">\(f(x)
:= \begin{pmatrix} \sin x \\ \cos x \end{pmatrix}\)</span>. Note that
the functions <code>sin()</code> and <code>cos()</code> are vectorised
and can handle inputs of arbitrary lengths.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">c</span>(<span class="fu">sin</span>(x), <span class="fu">cos</span>(x))</span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a><span class="fu">f</span>(<span class="dv">1</span>)</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a><span class="co">#&gt; [1] 0.8414710 0.5403023</span></span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="fu">jacobian</span>(f, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>)</span>
<span id="cb13-5"><a href="#cb13-5" tabindex="-1"></a><span class="co">#&gt;            [,1]       [,2]</span></span>
<span id="cb13-6"><a href="#cb13-6" tabindex="-1"></a><span class="co">#&gt; [1,]  0.5403023  0.0000000</span></span>
<span id="cb13-7"><a href="#cb13-7" tabindex="-1"></a><span class="co">#&gt; [2,]  0.0000000 -0.4161468</span></span>
<span id="cb13-8"><a href="#cb13-8" tabindex="-1"></a><span class="co">#&gt; [3,] -0.8414710  0.0000000</span></span>
<span id="cb13-9"><a href="#cb13-9" tabindex="-1"></a><span class="co">#&gt; [4,]  0.0000000 -0.9092974</span></span>
<span id="cb13-10"><a href="#cb13-10" tabindex="-1"></a><span class="fu">jacobian</span>(f, <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>)</span>
<span id="cb13-11"><a href="#cb13-11" tabindex="-1"></a><span class="co">#&gt;            [,1]       [,2]       [,3]</span></span>
<span id="cb13-12"><a href="#cb13-12" tabindex="-1"></a><span class="co">#&gt; [1,]  0.5403023  0.0000000  0.0000000</span></span>
<span id="cb13-13"><a href="#cb13-13" tabindex="-1"></a><span class="co">#&gt; [2,]  0.0000000 -0.4161468  0.0000000</span></span>
<span id="cb13-14"><a href="#cb13-14" tabindex="-1"></a><span class="co">#&gt; [3,]  0.0000000  0.0000000 -0.9899925</span></span>
<span id="cb13-15"><a href="#cb13-15" tabindex="-1"></a><span class="co">#&gt; [4,] -0.8414710  0.0000000  0.0000000</span></span>
<span id="cb13-16"><a href="#cb13-16" tabindex="-1"></a><span class="co">#&gt; [5,]  0.0000000 -0.9092974  0.0000000</span></span>
<span id="cb13-17"><a href="#cb13-17" tabindex="-1"></a><span class="co">#&gt; [6,]  0.0000000  0.0000000 -0.1411200</span></span></code></pre></div>
<p>The Jacobian of <span class="math inline">\(f\)</span> is <span class="math inline">\((\cos x, -\sin x)\)</span>; therefore, the
expected output for <code>x0 = 1:2</code> is <span class="math inline">\(\begin{pmatrix} \cos 1 &amp; \cos 2 \\ -\sin 1
&amp; -\sin 2 \end{pmatrix}\)</span>. However, the output is a matrix
made of stacked diagonal matrices, and the result is wrong even when
<span class="math inline">\(n\ne m\)</span>!</p>
<p>The root cause of this error is the manner in which the vector-valued
function is defined: the length of the object
<code>c(sin(x), cos(x))</code> depends on the length of <code>x</code>:
if <code>length(x) == n</code>, then,
<code>length(c(sin(x), cos(x))) == 2*n</code>, and <strong>R</strong>
sees the output as a vector, not a matrix! Hence,
<code>jacobian(f, x0)</code> computes <code>f(x0)</code> to check the
function dimension, infers from the output
<code>f(c(sin(1:2), cos(1:2)))</code> that <span class="math inline">\(m
= 4\)</span>, <span class="math inline">\(n = 2\)</span>, which is
wrong, and pre-allocates a <span class="math inline">\(4\times
2\)</span> matrix of zeros for the results. <span class="math inline">\(\blacksquare\)</span></p>
<p>Therefore, without any <em>a priori</em> information about the
function inputs and outputs, the function that computes matrices of
derivatives should follow the following logic for determining dimensions
and handling potential errors:</p>
<ul>
<li>If the input argument <code>x</code> is a scalar, prepare an
evaluation grid in the form of a list, evaluate <code>f</code> on that
grid, and combine the result by row. For safety, ignore the fact that
<code>f</code> may be vectorised (like <code>sin(1:100)</code>) and may
handle vector inputs in some cases.
<ul>
<li>If the resulting matrix has 1 column, then, the function is either
<span class="math inline">\(f_1\colon \mathbb{R} \mapsto
\mathbb{R}\)</span> or <span class="math inline">\(f_3\colon
\mathbb{R}^n \mapsto \mathbb{R}\)</span> (since the output has length
1). Proceed with computing the derivative approximation via weighted
sums and return the derivative estimate. If a Jacobian was initially
requested, throw a warning that the output of <code>f</code> is a scalar
and gradient facilities should be used.</li>
<li>If the resulting matrix has more than 1 column, then, the function
is either <span class="math inline">\(f_2\colon \mathbb{R} \mapsto
\mathbb{R}^m\)</span> or <span class="math inline">\(f_4\colon
\mathbb{R}^n \mapsto \mathbb{R}^m\)</span>. Proceed with computing the
Jacobian approximation via weighted sums and return the row matrix of
the Jacobian estimate. If a gradient was initially requested, throw a
warning that the output of <code>f</code> is not a scalar and Jacobian
facilities should be used, or return an error that this case should be
handled by the Jacobian routine directly.</li>
</ul></li>
<li>If the input argument <code>x</code> is a vector, first, it must be
determined if <code>f</code> is vectorised <span class="math inline">\(\vec f_1\colon \mathbb{R} \mapsto
\mathbb{R}\)</span> or not. Call <code>f(x)</code> and examine the
output.
<ul>
<li>If the output has the same length as the input, it is highly likely
that <code>f</code> is vectorised – for a counterexample where it is
not, see Example 1 above showing that if the output is a vector, an
input of identical length may create the impression of vectorisation by
pure chance. This case can be checked by calling <code>f(x[1])</code>
and checking if the output has length 1. If not, a warning that the user
should request a Jacobian for a vector-valued function should be issued.
If this check is passed, proceed by assuming vectorisation and prepare
the evaluation grid as a long vector to reduce overhead.</li>
<li>If the output has length 1, then, the function is certainly <span class="math inline">\(f_3\colon \mathbb{R}^n \mapsto
\mathbb{R}\)</span>, and <code>f</code> should be applied to a list of
input vectors for gradient approximation. If the user requested a
Jacobian, a warning should be shown.</li>
<li>If the output length is neither <code>length(x)</code> nor
<code>1</code>, then <code>f</code> definitely belongs to the class
<span class="math inline">\(f_4\colon \mathbb{R}^n \mapsto
\mathbb{R}^m\)</span>. In this case, similarly to the the case with
scalar output, a list of vectors should be prepared as the evaluation
grid for Jacobian approximation. If the user requested a gradient, throw
a warning that the result is a Jacobian matrix with non-null
dimensions.</li>
</ul></li>
</ul>
<p>The user should know whether their function is scalar- or
vector-valued and call the gradient or Jacobian routine accordingly.
Nevertheless, a check should be carried out for dimensionality and
vectorisation.</p>
<p><strong>Gradient</strong> vectorisation detection is implemented as
follows. Firstly, compute <code>f(x)</code>.</p>
<ul>
<li>If <code>length(x) == 1</code> and <code>length(f(x)) == 1</code>,
create the evaluation grid as a list and use <code>apply</code>-like
calls because there is no knowledge whether <code>f</code> is <span class="math inline">\(f_1\)</span> or <span class="math inline">\(\vec
f_1\)</span>.</li>
<li>If <code>length(x) &gt; 1</code> and <code>length(f(x)) == 1</code>,
create the evaluation grid as a list and use <code>apply</code>-like
calls. This is the <span class="math inline">\(f_3\)</span> case.</li>
<li>If <code>length(x) &gt; 1</code> and
<code>length(f(x)) &gt; 1</code>, check if
<code>length(f(x)) == length(x)</code>, create the evaluation grid as a
vector and call <code>f</code> on it directly (the <span class="math inline">\(\vec f_1\)</span> case). Otherwise, return an
error that a Jacobian should be computed instead because the input
vector length is not equal to the output vector length.</li>
<li>If <code>length(x) == 1</code> and <code>length(f(x)) &gt; 1</code>,
return an error indicating that a Jacobian should be computed instead.
<ul>
<li>Checking if <code>length(f(x[1])) == 1</code> to avoid the error
from Example 2 above is unreliable because certain functions (especially
statistical ones) are not defined for scalars and might return an error.
Example: a function that returns the deviations from the trimmed mean
where the highest and the lowest values are forcibly deleted could fail
if the number of observations is less than 3. Similarly, if the input
length determined the number of observations for a model, having a
single observation may result in non-invertibility of the model matrix;
this error would not occur for long vector inputs.</li>
</ul></li>
</ul>
<p><strong>Jacobians</strong> should be checked similarly: compute
<code>f(x)</code> and check the following.</p>
<ul>
<li>If <code>length(f(x)) == 1</code>, return an error indicating that a
gradient should be computed instead.</li>
<li>If <code>length(x) &gt; 1</code> and
<code>length(f(x)) &gt; 1</code>, create the evaluation grid as a list
and call <code>f</code> on it directly. This is the <span class="math inline">\(\vec f_4\)</span> case. Even if the components of
<span class="math inline">\(f\)</span> are vectorised, applying
<code>f</code> to a list of points is the only safe option.</li>
</ul>
</div>
</div>
<div id="breakdown-of-numderivgrad" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Breakdown of
<code>numDeriv::grad</code></h1>
<div id="handling-vectorised-inputs" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Handling vectorised
inputs</h2>
<p>The implementation of <code>numDeriv::grad()</code> is clever: it
allows both vectorised and non-vectorised functions to be provided as
the input argument. Since gradients are defined for scalar functions, it
has to check whether the function of interest is scalar- or vector
values. This can be non-trivial with vectorised functions: although
<span class="math inline">\(\sin x\)</span> is as scalar function,
<code>sin(1:100)</code> will return a vector in R. Therefore,
<code>numDeriv::grad()</code> has to determine if the
<em>implementation</em> of <span class="math inline">\(f\)</span> is
<span class="math inline">\(\mathbb{R}^n \mapsto \mathbb{R}^n\)</span>
(scalar / vectorised scalar), or <span class="math inline">\(\mathbb{R}^n \mapsto \mathbb{R}\)</span> (scalar
multivariate function), or neither. Hence, functions like <span class="math inline">\(\mathbb{R} \mapsto \mathbb{R}^n\)</span> or <span class="math inline">\(\mathbb{R}^n \mapsto \mathbb{R}^m\)</span> are not
considered valid inputs.</p>
<p>Internally, <code>numDeriv::grad(f, x0, ...)</code> computes
<code>f0 = f(x0, ...)</code> and checks the length of <code>f0</code>.
For the input argument <code>x0</code> with <code>length(x0) = n</code>,
the allowed lengths of <code>f0</code> are either <code>1</code> (scalar
output) or <code>n</code> (vectorised output). Expressions such as
<code>grad(function(x) c(sin(x), cos(x)), x = 1)</code> return an error,
implying that for vector-valued function, the user might want to compute
a Jacobian. Similarly, <code>numDeriv::hessian()</code> checks the
dimensions of <code>f0 = f(x0, ...)</code> and only allows
non-vectorised functions with <code>length(f0) == 1</code>. Finally, the
function <code>numDeriv::genD</code> computes the first- and
second-derivative information (without cross-derivatives).</p>
<p>This check is not without its drawbacks: it may miscalculate function
dimensions. The implementation in <code>pnd::Grad()</code> handles the
edge cases that cause wrong outputs in
<code>numDeriv::grad()</code>.</p>
<p>The <code>pnd</code> package provides similar functions:
<code>Grad()</code> is a drop-in replacement for
<code>numDeriv::grad()</code>, <code>Hessian()</code> replaces
<code>numDeriv::hessian()</code>, <code>Jacobian()</code> subsumes
<code>numDeriv::jacobian()</code>, and <code>GenD()</code>, whilst not
corresponding to <code>numDeriv::genD()</code>, is the real workhorse
that computes arbitrary derivatives of arbitrary accuracy order.</p>
<p><strong>Example 1: correct dimensionality check results for
vector-valued functions.</strong></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>f2 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">c</span>(<span class="fu">sin</span>(x), <span class="fu">cos</span>(x))  <span class="co"># Vector output -&gt; gradient is unsupported</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a><span class="fu">grad</span>(f2, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a><span class="co">#&gt; Error in grad.default(f2, x = 1:4): grad assumes a scalar valued function.</span></span>
<span id="cb14-4"><a href="#cb14-4" tabindex="-1"></a><span class="fu">hessian</span>(f2, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb14-5"><a href="#cb14-5" tabindex="-1"></a><span class="co">#&gt; Error in hessian.default(f2, x = 1:4): Richardson method for hessian assumes a scalar valued function.</span></span>
<span id="cb14-6"><a href="#cb14-6" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" tabindex="-1"></a><span class="fu">Grad</span>(f2, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb14-8"><a href="#cb14-8" tabindex="-1"></a><span class="co">#&gt; Error in Grad(f2, x = 1:4): Use &#39;Jacobian()&#39; instead of &#39;Grad()&#39; for vector-valued functions to obtain a matrix of derivatives.</span></span></code></pre></div>
<p>This check correctly identifies non-vectorised functions as well:</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a>f2 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">c</span>(<span class="fu">sum</span>(<span class="fu">sin</span>(x)), <span class="fu">sum</span>(<span class="fu">cos</span>(x)))</span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="fu">grad</span>(f2, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="co">#&gt; Error in grad.default(f2, x = 1:4): grad assumes a scalar valued function.</span></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a><span class="fu">hessian</span>(f2, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a><span class="co">#&gt; Error in hessian.default(f2, x = 1:4): Richardson method for hessian assumes a scalar valued function.</span></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a><span class="fu">jacobian</span>(f2, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a><span class="co">#&gt;            [,1]       [,2]       [,3]       [,4]</span></span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a><span class="co">#&gt; [1,]  0.5403023 -0.4161468 -0.9899925 -0.6536436</span></span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a><span class="co">#&gt; [2,] -0.8414710 -0.9092974 -0.1411200  0.7568025</span></span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" tabindex="-1"></a><span class="fu">Grad</span>(f2, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb15-12"><a href="#cb15-12" tabindex="-1"></a><span class="co">#&gt; Error in Grad(f2, x = 1:4): Use &#39;Jacobian()&#39; instead of &#39;Grad()&#39; for vector-valued functions to obtain a matrix of derivatives.</span></span>
<span id="cb15-13"><a href="#cb15-13" tabindex="-1"></a><span class="fu">Jacobian</span>(f2, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb15-14"><a href="#cb15-14" tabindex="-1"></a><span class="co">#&gt; Estimated Jacobian:</span></span>
<span id="cb15-15"><a href="#cb15-15" tabindex="-1"></a><span class="co">#&gt;  0.5403  -0.4161  -0.9900  -0.6536</span></span>
<span id="cb15-16"><a href="#cb15-16" tabindex="-1"></a><span class="co">#&gt; -0.8415  -0.9093  -0.1411   0.7568</span></span>
<span id="cb15-17"><a href="#cb15-17" tabindex="-1"></a><span class="co">#&gt; (default step size range: 6.1e-06...2.4e-05.)</span></span></code></pre></div>
<p><strong>Example 2: valid input, invalid output in
<code>numDeriv</code>.</strong></p>
<p>If a function consists of individually vectorised components and
returns an output that differs in length from the input, then,
<code>numDeriv::jacobian</code> may return wrong results. Specifically,
the output should be stacked coordinate-wise gradients, but is made of
stacked diagonal matrices instead.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>f2 <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">c</span>(<span class="fu">sin</span>(x), <span class="fu">cos</span>(x))</span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a><span class="fu">grad</span>(f2, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb16-3"><a href="#cb16-3" tabindex="-1"></a><span class="co">#&gt; Error in grad.default(f2, x = 1:4): grad assumes a scalar valued function.</span></span>
<span id="cb16-4"><a href="#cb16-4" tabindex="-1"></a><span class="fu">jacobian</span>(f2, <span class="at">x =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>)</span>
<span id="cb16-5"><a href="#cb16-5" tabindex="-1"></a><span class="co">#&gt;            [,1]       [,2]       [,3]       [,4]</span></span>
<span id="cb16-6"><a href="#cb16-6" tabindex="-1"></a><span class="co">#&gt; [1,]  0.5403023  0.0000000  0.0000000  0.0000000</span></span>
<span id="cb16-7"><a href="#cb16-7" tabindex="-1"></a><span class="co">#&gt; [2,]  0.0000000 -0.4161468  0.0000000  0.0000000</span></span>
<span id="cb16-8"><a href="#cb16-8" tabindex="-1"></a><span class="co">#&gt; [3,]  0.0000000  0.0000000 -0.9899925  0.0000000</span></span>
<span id="cb16-9"><a href="#cb16-9" tabindex="-1"></a><span class="co">#&gt; [4,]  0.0000000  0.0000000  0.0000000 -0.6536436</span></span>
<span id="cb16-10"><a href="#cb16-10" tabindex="-1"></a><span class="co">#&gt; [5,] -0.8414710  0.0000000  0.0000000  0.0000000</span></span>
<span id="cb16-11"><a href="#cb16-11" tabindex="-1"></a><span class="co">#&gt; [6,]  0.0000000 -0.9092974  0.0000000  0.0000000</span></span>
<span id="cb16-12"><a href="#cb16-12" tabindex="-1"></a><span class="co">#&gt; [7,]  0.0000000  0.0000000 -0.1411200  0.0000000</span></span>
<span id="cb16-13"><a href="#cb16-13" tabindex="-1"></a><span class="co">#&gt; [8,]  0.0000000  0.0000000  0.0000000  0.7568025</span></span></code></pre></div>
</div>
<div id="approximation-method" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Approximation
method</h2>
<p>There are 3 methods implemented in <code>numDeriv</code> for gradient
calculation: ‘simple’, ‘Richardson’, and ‘complex’. For simplicity, the
formulæ given here assume scalar arguments; for vector arguments, these
calculations are done coordinate by coordinate.</p>
<ul>
<li><code>method = &quot;simple&quot;</code>, <code>numDeriv::grad()</code>
computes a fast one-sided difference with a fixed step size (the default
is 0.0001): <span class="math inline">\(f&#39;_{\mathrm{simple}} :=
\frac{f(x + h) - f(x)}{h}\)</span> or <span class="math inline">\(\frac{f(x) - f(x-h)}{h}\)</span>.</li>
<li><code>method = &quot;Richardson&quot;</code>, calls a routine for Richardson
extrapolation (described above).</li>
<li><code>method = &quot;complex&quot;</code> is valid only for those functions
for which the function may take complex inputs and handle complex
outputs. This limits the scope of this method to well-defined convenient
mathematical functions; this excludes the majority of applications in
statistics, biometrics, economics, and data science. However, if there
is such a function, then,
<code>numDeriv::grad(..., method = &quot;complex&quot;)</code> will return <span class="math inline">\(\Im[f(x + \mathrm{i} h)]/h\)</span>.</li>
</ul>
<p>This syntax of <code>numDeriv::grad</code> makes consistent step size
selection a chore because of the differences in method implementations.
If <code>method = &quot;simple&quot;</code>, the step size is
<code>method.args$eps</code> (defaults to <span class="math inline">\(10^{-4}\)</span>), and it is absolute. If
<code>method = &quot;Richardson&quot;</code>, the step size is the initial step
size in the Richardson extrapolation, and is computed as follows:</p>
<ul>
<li>If <span class="math inline">\(x \ge \mathrm{zero.tol}\)</span>,
then, the step size is relative, equal to <span class="math inline">\(h
= d|x|\)</span> with default <span class="math inline">\(d =
10^{-4}\)</span> and <span class="math inline">\(\mathrm{zero.tol} =
\sqrt{\epsilon_{\mathrm{mach}} / (7 \cdot 10^{-7})} \approx 1.8 \cdot
10^{-5}\)</span>;</li>
<li>If <span class="math inline">\(x &lt; \mathrm{zero.tol}\)</span>,
then, an absolute step is added to the relative one, resulting in the
effective step size <span class="math inline">\(h = d|x| +
\epsilon\)</span> with default <span class="math inline">\(\epsilon =
10^{-1}\)</span>.</li>
<li>A succession of finite differences is computed <span class="math inline">\(r\)</span> times (default <span class="math inline">\(r = 4\)</span>) where at each step, <span class="math inline">\(h\)</span> is reduced by a factor of <span class="math inline">\(v\)</span> (default <span class="math inline">\(v
= 2\)</span>).</li>
</ul>
<p>Technical remarks.</p>
<ul>
<li>If the derivative is one-sided, then, the step size is doubled.</li>
<li>If the finite difference value at any step is less than <span class="math inline">\(10^{-20}\)</span> in absolute value, it is set to
zero.</li>
</ul>
</div>
</div>
<div id="compatibility-implies-syntax-support-not-identical-values" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Compatibility implies
syntax support, not identical values</h1>
<div id="numderiv-zero-handling-has-a-discontinuity" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> numDeriv zero
handling has a discontinuity</h2>
<p>In <code>numDeriv</code>, the default step size – arguably the most
important tuning parameter of numerical differences – is equal to
<code>d*x + (abs(x)&lt;zero.tol) * eps</code>, where
<code>= 1e-4</code>, <code>d = 1e-4</code>,
<code>zero.tol = sqrt(.Machine$double.eps/7e-7)</code> (approximately
<code>1.8e-5</code>).</p>
<ul>
<li>The manual states, ‘<code>eps</code> is used instead of
<code>d</code> for elements of <code>x</code> which are zero’, but the
implementation is different: if <code>x</code> is just below
<code>zero.tol</code>, the actual step is not <code>eps</code> but
<code>d*zero.tol + eps</code>.</li>
<li>If <code>x</code> transitions from being below just
<code>zero.tol</code> to being just above it, the step size changes from
<code>d*zero.tol + eps</code> to <code>d*zero.tol</code>, i.e. it
decreases, which creates a discontinuity and a counter-intuitive
result.</li>
</ul>
<p>The second point can be illustrated with an example where the step
size for <code>x = 1.7e-5</code> exceeds that for
<code>x = 1.8e-5</code>, which makes little sense mathematically:</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {<span class="fu">cat</span>(x, <span class="st">&quot; &quot;</span>); <span class="fu">sin</span>(x)}</span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a><span class="fu">grad</span>(f, <span class="fl">1.7e-5</span>, <span class="at">method.args =</span> <span class="fu">list</span>(<span class="at">r =</span> <span class="dv">2</span>))  <span class="co"># step 1.0e-4</span></span>
<span id="cb17-3"><a href="#cb17-3" tabindex="-1"></a><span class="co">#&gt; 1.7e-05  0.0001170017  -8.30017e-05  6.700085e-05  -3.300085e-05</span></span>
<span id="cb17-4"><a href="#cb17-4" tabindex="-1"></a><span class="co">#&gt; [1] 1</span></span>
<span id="cb17-5"><a href="#cb17-5" tabindex="-1"></a><span class="fu">grad</span>(f, <span class="fl">1.8e-5</span>, <span class="at">method.args =</span> <span class="fu">list</span>(<span class="at">r =</span> <span class="dv">2</span>))  <span class="co"># step 1.8e-9</span></span>
<span id="cb17-6"><a href="#cb17-6" tabindex="-1"></a><span class="co">#&gt; 1.8e-05  1.80018e-05  1.79982e-05  1.80009e-05  1.79991e-05</span></span>
<span id="cb17-7"><a href="#cb17-7" tabindex="-1"></a><span class="co">#&gt; [1] 1</span></span></code></pre></div>
<p>In <code>pnd</code>, the default step size is decided depending on
the value of <code>x</code>:</p>
<ol style="list-style-type: decimal">
<li>For <code>x &lt; zero.tol</code>, it is constant and equal to
<code>zero.tol</code>, where the revised value of <code>zero.tol</code>
is <code>sqrt(.Machine$epsilon)</code>, or <code>1.5e-8</code>;</li>
<li>For <code>x &gt; 1</code>, the step size is equal to <code>x</code>
times the machine epsilon to the appropriate power (inverse of the sum
of derivative and accuracy orders);</li>
<li>For <code>zero.tol &lt;= x &lt;= 1</code>, the step size is
interpolated exponentially between <code>zero.tol</code> and
<code>macheps^(1 / (deriv.order + acc.order))</code>.</li>
</ol>
<p>The chosen value of <code>zero.tol</code> is reasonable in many
applications in biostatistics and econometrics where non-negativity of
certain parameters in optimisation is often achieved via box constraints
with a comparable distance from the boundary.</p>
<p>The default step size as a function of <code>x</code> is illustrated
below.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" tabindex="-1"></a>xseq <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">10</span>, <span class="dv">2</span>, <span class="fl">0.25</span>)</span>
<span id="cb18-2"><a href="#cb18-2" tabindex="-1"></a>sseq2 <span class="ot">&lt;-</span> <span class="fu">stepx</span>(xseq, <span class="at">acc.order =</span> <span class="dv">2</span>)</span>
<span id="cb18-3"><a href="#cb18-3" tabindex="-1"></a>sseq4 <span class="ot">&lt;-</span> <span class="fu">stepx</span>(xseq, <span class="at">acc.order =</span> <span class="dv">4</span>)</span>
<span id="cb18-4"><a href="#cb18-4" tabindex="-1"></a><span class="fu">matplot</span>(xseq, <span class="fu">cbind</span>(sseq2, sseq4), <span class="at">lty =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="at">col =</span> <span class="dv">1</span>, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">bty =</span> <span class="st">&quot;n&quot;</span>,</span>
<span id="cb18-5"><a href="#cb18-5" tabindex="-1"></a>        <span class="at">log =</span> <span class="st">&quot;xy&quot;</span>, <span class="at">main =</span> <span class="st">&quot;Default step size&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Point&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Step&quot;</span>)</span>
<span id="cb18-6"><a href="#cb18-6" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="fu">c</span>(<span class="st">&quot;2nd-order accurate&quot;</span>, <span class="st">&quot;4th-order accurate&quot;</span>), <span class="at">lty =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>)</span></code></pre></div>
<p><img role="img" src="data:image/png;base64,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" width="10cm" /></p>
</div>
<div id="not-all-evaluations-matter-for-richardson-extrapolation" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Not all evaluations
matter for Richardson extrapolation</h2>
<p>A detailed description of the Richardson extrapolation is given in
<span class="citation">Ridders (1982)</span>. To dissect the internals,
it suffices to make the function print its input arguments.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {<span class="fu">cat</span>(x, <span class="st">&quot; &quot;</span>); <span class="fu">sin</span>(x)}</span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>x0 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb19-3"><a href="#cb19-3" tabindex="-1"></a>g1 <span class="ot">&lt;-</span> numDeriv<span class="sc">::</span><span class="fu">grad</span>(f, x0)</span>
<span id="cb19-4"><a href="#cb19-4" tabindex="-1"></a><span class="co">#&gt; 1  1.0001  0.9999  1.00005  0.99995  1.000025  0.999975  1.000012  0.9999875</span></span>
<span id="cb19-5"><a href="#cb19-5" tabindex="-1"></a><span class="fu">print</span>(g1)</span>
<span id="cb19-6"><a href="#cb19-6" tabindex="-1"></a><span class="co">#&gt; [1] 0.5403023</span></span>
<span id="cb19-7"><a href="#cb19-7" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Auto-detected step:&quot;</span>, <span class="fu">step.SW</span>(sin, x0)<span class="sc">$</span>par, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb19-9"><a href="#cb19-9" tabindex="-1"></a><span class="co">#&gt; Auto-detected step: 5e-06</span></span>
<span id="cb19-10"><a href="#cb19-10" tabindex="-1"></a>hgrid <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="sc">-</span><span class="dv">10</span>, <span class="sc">-</span><span class="dv">4</span>, <span class="dv">1</span><span class="sc">/</span><span class="dv">32</span>)</span>
<span id="cb19-11"><a href="#cb19-11" tabindex="-1"></a>errors <span class="ot">&lt;-</span> <span class="fu">sapply</span>(hgrid, <span class="cf">function</span>(h) <span class="fu">Grad</span>(sin, x0, <span class="at">h =</span> h, <span class="at">cores =</span> <span class="dv">1</span>,</span>
<span id="cb19-12"><a href="#cb19-12" tabindex="-1"></a>            <span class="at">elementwise =</span> <span class="cn">TRUE</span>, <span class="at">vectorised =</span> <span class="cn">TRUE</span>, <span class="at">multivalued =</span> <span class="cn">FALSE</span>)) <span class="sc">-</span> <span class="fu">cos</span>(x0)</span>
<span id="cb19-13"><a href="#cb19-13" tabindex="-1"></a><span class="fu">plot</span>(hgrid, <span class="fu">abs</span>(errors), <span class="at">log =</span> <span class="st">&quot;xy&quot;</span>, <span class="at">cex =</span> <span class="fl">0.6</span>)</span></code></pre></div>
<p><img role="img" src="data:image/png;base64,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" width="10cm" /></p>
<p>Even with 4 Richardson iterations, the default final step size is
<code>1.25e-5</code>, which is larger than the optimal step size, which
is less than <code>1e-5</code> (visible from the plot), the empirically
determined step size <code>5e-6</code>, or even the rule-of-thumb value
<code>MachEps^(1/3)</code>. The equivalence between
<code>numDeriv</code> and <code>pnd</code> implementations of the
gradient (the latter being more general) can be established by computing
the optimal weights in the finite difference for the points in the
iterations of the Richardson extrapolation.</p>
<p>In this example, the default sequence of steps is <span class="math inline">\(\{h_i\}_{i=1}^4 = 10^{-4} / \{1, 2, 4,
8\}\)</span>, and the resulting stencil is <span class="math inline">\(\{\pm h_i\}_{i=1}^4\)</span>. The respective
weights can be calculated via <code>fdCoef()</code>:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" tabindex="-1"></a>b <span class="ot">&lt;-</span> <span class="fu">fdCoef</span>(<span class="at">stencil =</span> <span class="fu">c</span>(<span class="sc">-</span>(<span class="dv">2</span><span class="sc">^</span>(<span class="dv">3</span><span class="sc">:</span><span class="dv">0</span>)), <span class="dv">2</span><span class="sc">^</span>(<span class="dv">0</span><span class="sc">:</span><span class="dv">3</span>)))</span>
<span id="cb20-2"><a href="#cb20-2" tabindex="-1"></a><span class="fu">print</span>(b)</span>
<span id="cb20-3"><a href="#cb20-3" tabindex="-1"></a><span class="co">#&gt; $stencil</span></span>
<span id="cb20-4"><a href="#cb20-4" tabindex="-1"></a><span class="co">#&gt; [1] -8 -4 -2 -1  1  2  4  8</span></span>
<span id="cb20-5"><a href="#cb20-5" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb20-6"><a href="#cb20-6" tabindex="-1"></a><span class="co">#&gt; $weights</span></span>
<span id="cb20-7"><a href="#cb20-7" tabindex="-1"></a><span class="co">#&gt;          x-8h          x-4h          x-2h          x-1h          x+1h </span></span>
<span id="cb20-8"><a href="#cb20-8" tabindex="-1"></a><span class="co">#&gt;  2.204586e-05 -3.703704e-03  1.185185e-01 -7.223986e-01  7.223986e-01 </span></span>
<span id="cb20-9"><a href="#cb20-9" tabindex="-1"></a><span class="co">#&gt;          x+2h          x+4h          x+8h </span></span>
<span id="cb20-10"><a href="#cb20-10" tabindex="-1"></a><span class="co">#&gt; -1.185185e-01  3.703704e-03 -2.204586e-05 </span></span>
<span id="cb20-11"><a href="#cb20-11" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb20-12"><a href="#cb20-12" tabindex="-1"></a><span class="co">#&gt; attr(,&quot;remainder.coef&quot;)</span></span>
<span id="cb20-13"><a href="#cb20-13" tabindex="-1"></a><span class="co">#&gt; [1] -0.01128748</span></span>
<span id="cb20-14"><a href="#cb20-14" tabindex="-1"></a><span class="co">#&gt; attr(,&quot;accuracy.order&quot;)</span></span>
<span id="cb20-15"><a href="#cb20-15" tabindex="-1"></a><span class="co">#&gt; requested effective </span></span>
<span id="cb20-16"><a href="#cb20-16" tabindex="-1"></a><span class="co">#&gt;        NA         8 </span></span>
<span id="cb20-17"><a href="#cb20-17" tabindex="-1"></a><span class="co">#&gt; attr(,&quot;expansion&quot;)</span></span>
<span id="cb20-18"><a href="#cb20-18" tabindex="-1"></a><span class="co">#&gt; [1] &quot;f &#39; - 1.1287e-02 f^(9) + ...&quot;</span></span></code></pre></div>
<p>Here, the weights on the outermost points – the first step of the
iteration with <span class="math inline">\(h_1\)</span> – are minuscule
(<code>2.2e-5</code>). The relative importance of each iteration for
this specific function at this specific point can be found from the
equation <span class="math display">\[\begin{multline*}
f&#39;_{\mathrm{Rich,8}} = \sum_{i=1}^4 w_{i}[f(x + h_i) - f(x + h_i)],
\end{multline*}\]</span> where <span class="math inline">\(\{w_i\}_{i=1}^4 \approx \{+0.608, -0.0997,
+0.0031, -0.00002\}\)</span>. These values are available as a length
analytical expression with denominator 45360, but in this case, they
were calculated numerically:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a>fd <span class="ot">&lt;-</span> <span class="fu">sin</span>(x0 <span class="sc">+</span> b<span class="sc">$</span>stencil <span class="sc">/</span> <span class="dv">8</span> <span class="sc">*</span> <span class="fl">1e-4</span>) <span class="sc">*</span> b<span class="sc">$</span>weights</span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a><span class="fu">abs</span>(fd[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>]) <span class="sc">/</span> <span class="fu">sum</span>(<span class="fu">abs</span>(fd[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>]))</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a><span class="co">#&gt;         x-8h         x-4h         x-2h         x-1h </span></span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a><span class="co">#&gt; 2.609937e-05 4.384834e-03 1.403170e-01 8.552721e-01</span></span></code></pre></div>
<p>The absolute contribution of each term is proportional to specific
terms of a certain polynomial. Practically, it implies that the weights
of the summation terms further away from the point of interest decay
exponentially (up to a certain constant), and similar accuracy can be
achieved with fewer evaluations. In this example, 85.5% of the sum is
defined by the finite difference with <span class="math inline">\(h_4\)</span>, and 14.0% with <span class="math inline">\(h_3\)</span>. Therefore, the following function is
twice as cheap, and the difference between the estimated derivative
values is negligible.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" tabindex="-1"></a>g2 <span class="ot">&lt;-</span> <span class="fu">Grad</span>(f, x0, <span class="at">h =</span> <span class="fl">1.25e-05</span>, <span class="at">acc.order =</span> <span class="dv">4</span>,</span>
<span id="cb22-2"><a href="#cb22-2" tabindex="-1"></a>           <span class="at">elementwise =</span> <span class="cn">TRUE</span>, <span class="at">vectorised =</span> <span class="cn">TRUE</span>, <span class="at">multivalued =</span> <span class="cn">FALSE</span>)</span>
<span id="cb22-3"><a href="#cb22-3" tabindex="-1"></a><span class="co">#&gt; 0.999975  0.9999875  1.000012  1.000025</span></span>
<span id="cb22-4"><a href="#cb22-4" tabindex="-1"></a><span class="fu">print</span>(g2)</span>
<span id="cb22-5"><a href="#cb22-5" tabindex="-1"></a><span class="co">#&gt; Estimated derivative: 0.5403</span></span>
<span id="cb22-6"><a href="#cb22-6" tabindex="-1"></a><span class="co">#&gt; (user-supplied step size: 1.3e-05).</span></span>
<span id="cb22-7"><a href="#cb22-7" tabindex="-1"></a></span>
<span id="cb22-8"><a href="#cb22-8" tabindex="-1"></a><span class="fu">c</span>(<span class="at">diff =</span> g1 <span class="sc">-</span> g2, <span class="at">Error8 =</span> <span class="fu">cos</span>(x0) <span class="sc">-</span> g1, <span class="at">Error4 =</span> <span class="fu">cos</span>(x0) <span class="sc">-</span> g2)</span>
<span id="cb22-9"><a href="#cb22-9" tabindex="-1"></a><span class="co">#&gt;          diff        Error8        Error4 </span></span>
<span id="cb22-10"><a href="#cb22-10" tabindex="-1"></a><span class="co">#&gt; -2.377543e-12  4.636624e-12  2.259082e-12</span></span></code></pre></div>
<p>In this example, the approximation errors from the Richardson
extrapolation and a much cheaper weighted sum have the same order of
magnitude. By not-so-unlikely coincidence, the actual error of
<code>Grad</code> is less than that of <code>grad</code>, which is due
to the unpredictable numerical error that is not dominated by the
bounded higher-order derivatives truncated in the 4th-order-accurate
derivative.</p>
<p>Choosing a large initial step and subsequent shrinking are wasteful
because similar, if not practically equivalent, accuracy can be achieved
with twice as few evaluations with a reasonably chosen step size. If the
function is to be evaluated many times, selecting the optimal step size
via a data-driven procedure not only saves time whilst attaining
comparable accuracy but also provides opportunities for speed-ups via
parallel evaluation of the function on a grid. Richardson extrapolation,
on the other hand, is typically computed in a loop, and since its fully
parallelised implementation is fully subsumed by the weighted-sum
approach, the new package does not dedicate any special numerical
routine to this particular case.</p>
<p>Finally, the choice of an optimal non-uniform evaluation grid for a
specific function at a specific point remains an open question. One
short answer would be ‘the higher the curvature, the smaller the step’,
which is the principle used in plotting software to trace the most
accurate paths with the fewest evaluations. Another answer comes from
the field of optimal polynomial approximation and involves such
solutions as Chebyshev polynomials, Padé approximants, and most
importantly, the Remez algorithm. Applying these methods to numerical
derivatives requires careful handling of differences because the goal is
approximating an unknown function that is evaluated with a rounding
error, therefore, at least an arbitrary-precision numerical tool must be
used in evaluating the derivatives.</p>
</div>
</div>
<div id="diagnostics" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Diagnostics</h1>
<div id="higher-order-accuracy-diagnostics" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Higher-order accuracy
diagnostics</h2>
<p>The term ‘extrapolation group’ can be found, e.g., in <span class="citation">Lindfield and Penny (1989)</span>, where it is used in
sections on numerical integration and differentiation and describes
sub-intervals on which polynomials of different degrees are fitted to
improve approximation accuracy. Computer programmes in the 1980s were
often optimised for memory use and size, not necessarily for ease of
interpretation or transparency of their correspondence to the
theoretical relationships that they were translating into numbers.
Therefore, a reader might get confused by the terms ‘extrapolation
group’, ‘improvement iteration’, and ‘accuracy order’. The
<code>pnd</code> package aims to provide more straightforward diagnostic
information.</p>
<p>Suppose that one wants to compute the derivative of <span class="math inline">\(f(x) = x^7\)</span> at <span class="math inline">\(x_0 = 1\)</span> – therefore, the true derivative
value is <span class="math inline">\(f&#39;(1) = 6\)</span>. The
following code effectively debugs <code>numDeriv::grad()</code> and
shows how many values are computed (we choose the initial step size
<span class="math inline">\(h=2^{-10} = 1/1024\)</span> to avoid any
representation error of <span class="math inline">\(x_0+h\)</span>):</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x) {<span class="fu">print</span>(x, <span class="at">digits =</span> <span class="dv">16</span>); x<span class="sc">^</span><span class="dv">9</span>} </span>
<span id="cb23-2"><a href="#cb23-2" tabindex="-1"></a>fp1 <span class="ot">&lt;-</span> numDeriv<span class="sc">::</span><span class="fu">grad</span>(f, <span class="at">x =</span> <span class="dv">1</span>, <span class="at">method.args =</span> <span class="fu">list</span>(<span class="at">r =</span> <span class="dv">4</span>, <span class="at">d =</span> <span class="dv">2</span><span class="sc">^-</span><span class="dv">10</span>, <span class="at">show.details =</span> <span class="cn">TRUE</span>))</span>
<span id="cb23-3"><a href="#cb23-3" tabindex="-1"></a><span class="co">#&gt; [1] 1</span></span>
<span id="cb23-4"><a href="#cb23-4" tabindex="-1"></a><span class="co">#&gt; [1] 1.0009765625</span></span>
<span id="cb23-5"><a href="#cb23-5" tabindex="-1"></a><span class="co">#&gt; [1] 0.9990234375</span></span>
<span id="cb23-6"><a href="#cb23-6" tabindex="-1"></a><span class="co">#&gt; [1] 1.00048828125</span></span>
<span id="cb23-7"><a href="#cb23-7" tabindex="-1"></a><span class="co">#&gt; [1] 0.99951171875</span></span>
<span id="cb23-8"><a href="#cb23-8" tabindex="-1"></a><span class="co">#&gt; [1] 1.000244140625</span></span>
<span id="cb23-9"><a href="#cb23-9" tabindex="-1"></a><span class="co">#&gt; [1] 0.999755859375</span></span>
<span id="cb23-10"><a href="#cb23-10" tabindex="-1"></a><span class="co">#&gt; [1] 1.0001220703125</span></span>
<span id="cb23-11"><a href="#cb23-11" tabindex="-1"></a><span class="co">#&gt; [1] 0.9998779296875</span></span>
<span id="cb23-12"><a href="#cb23-12" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb23-13"><a href="#cb23-13" tabindex="-1"></a><span class="co">#&gt;  first order approximations </span></span>
<span id="cb23-14"><a href="#cb23-14" tabindex="-1"></a><span class="co">#&gt;               [,1]</span></span>
<span id="cb23-15"><a href="#cb23-15" tabindex="-1"></a><span class="co">#&gt; [1,] 9.00008010876</span></span>
<span id="cb23-16"><a href="#cb23-16" tabindex="-1"></a><span class="co">#&gt; [2,] 9.00002002717</span></span>
<span id="cb23-17"><a href="#cb23-17" tabindex="-1"></a><span class="co">#&gt; [3,] 9.00000500679</span></span>
<span id="cb23-18"><a href="#cb23-18" tabindex="-1"></a><span class="co">#&gt; [4,] 9.00000125170</span></span>
<span id="cb23-19"><a href="#cb23-19" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb23-20"><a href="#cb23-20" tabindex="-1"></a><span class="co">#&gt;  Richarson improvement group No.  1 </span></span>
<span id="cb23-21"><a href="#cb23-21" tabindex="-1"></a><span class="co">#&gt;               [,1]</span></span>
<span id="cb23-22"><a href="#cb23-22" tabindex="-1"></a><span class="co">#&gt; [1,] 8.99999999997</span></span>
<span id="cb23-23"><a href="#cb23-23" tabindex="-1"></a><span class="co">#&gt; [2,] 9.00000000000</span></span>
<span id="cb23-24"><a href="#cb23-24" tabindex="-1"></a><span class="co">#&gt; [3,] 9.00000000000</span></span>
<span id="cb23-25"><a href="#cb23-25" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb23-26"><a href="#cb23-26" tabindex="-1"></a><span class="co">#&gt;  Richarson improvement group No.  2 </span></span>
<span id="cb23-27"><a href="#cb23-27" tabindex="-1"></a><span class="co">#&gt;      [,1]</span></span>
<span id="cb23-28"><a href="#cb23-28" tabindex="-1"></a><span class="co">#&gt; [1,]    9</span></span>
<span id="cb23-29"><a href="#cb23-29" tabindex="-1"></a><span class="co">#&gt; [2,]    9</span></span>
<span id="cb23-30"><a href="#cb23-30" tabindex="-1"></a><span class="fu">print</span>(fp1, <span class="at">digits =</span> <span class="dv">16</span>)</span>
<span id="cb23-31"><a href="#cb23-31" tabindex="-1"></a><span class="co">#&gt; [1] 9.000000000000064</span></span></code></pre></div>
<p>In total, the function was called 9 times: 1 for the initial
dimensionality check and 2 times per each of the 4 iterations (since ).
In this implementation, the terminology differs from the textbook that
it is referring to: <code>numDeriv</code>’s ‘first-order approximations’
correspond to Lindfield &amp; Penny’s ‘Group 1’, ‘Richarsdon improvement
group No. 1’ to LP’s ‘Group 2’, and the final output returned by
<code>grad()</code> would be ‘Group 4’.</p>
<p>In central differences, <span class="math inline">\(f(x_0)\)</span>
is not used; with 8 symmetric evaluations, the truncation error of the
result is <span class="math inline">\(O(h^8)\)</span>.
<strong>Therefore, the standard textbook results about the optimal step
size for central first differences being proportional to</strong> <span class="math inline">\(\epsilon_{\mathrm{mach}}^{1/3}\)</span>
<strong>are not applicable to</strong> <code>numDeriv::grad()</code>
<strong>because the default accuracy order is 8 and the step size of the
order</strong> <span class="math inline">\(\epsilon_{\mathrm{mach}}^{1/3}\)</span> <strong>is
appropriate in this case</strong>.</p>
<p>With <code>pnd</code>, there is no need to re-define the function to
save its computed values because both the grid and the function values
are returned as the attribute.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="co"># f &lt;- function(x) x^9</span></span>
<span id="cb24-2"><a href="#cb24-2" tabindex="-1"></a><span class="co"># fp2 &lt;- pnd::Grad(f, x = 1, h = &quot;SW&quot;, acc.order = 8, vectorised1d = TRUE)</span></span>
<span id="cb24-3"><a href="#cb24-3" tabindex="-1"></a><span class="co"># print(attributes(fp2)$step.search$iterations, digits = 16)</span></span></code></pre></div>
<p>This highlights an important difference between the two packages:</p>
<ul>
<li><code>numDeriv::grad()</code> starts at a relatively large step size
<span class="math inline">\(h = 10^{-4} \cdot \max(|x_0|, 1)\)</span>
and repeatedly shrinks it, yielding an exponentially decreasing sequence
<span class="math inline">\(h, h / v, h / v^2, \ldots\)</span> using the
same reduction factor <span class="math inline">\(v\)</span> to attain
the desired accuracy order <span class="math inline">\(a = 2v\)</span> –
<strong>slow due to many evaluations, but rather accurate</strong>;</li>
<li><code>pnd::Grad()</code> starts at the rule-of-thumb step size of
the correct order <span class="math inline">\(\epsilon_{\text{mach}}^{1/(1+a)}\)</span>. By
default, it computes central derivatives <span class="math inline">\(f&#39;_{\mathrm{CD}}(x, h)\)</span> with accuracy
order <span class="math inline">\(a = 2\)</span> – <strong>fast due to
fewer evaluations, smaller default step size that agrees with numerical
analysis literature, estimates of the approximation error are
provided</strong>. If accuracy order <span class="math inline">\(a &gt;
2\)</span> is requested, then, the evaluation grid is linear: <span class="math inline">\(x \pm h, x \pm 2h, x \pm 3h, \ldots\)</span>.</li>
</ul>
<p>Finally, the method argument <code>show.details = TRUE</code> is
ignored for Hessians in <code>numDeriv</code>:</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" tabindex="-1"></a>g <span class="ot">&lt;-</span> <span class="cf">function</span>(x) <span class="fu">sum</span>(<span class="fu">sin</span>(x))</span>
<span id="cb25-2"><a href="#cb25-2" tabindex="-1"></a><span class="fu">hessian</span>(g, <span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="at">method.args =</span> <span class="fu">list</span>(<span class="at">show.details =</span> <span class="cn">TRUE</span>))</span>
<span id="cb25-3"><a href="#cb25-3" tabindex="-1"></a><span class="co">#&gt;               [,1]          [,2]          [,3]</span></span>
<span id="cb25-4"><a href="#cb25-4" tabindex="-1"></a><span class="co">#&gt; [1,] -8.414710e-01  4.072455e-14 -2.864747e-13</span></span>
<span id="cb25-5"><a href="#cb25-5" tabindex="-1"></a><span class="co">#&gt; [2,]  4.072455e-14 -9.092974e-01  1.358524e-14</span></span>
<span id="cb25-6"><a href="#cb25-6" tabindex="-1"></a><span class="co">#&gt; [3,] -2.864747e-13  1.358524e-14 -1.411200e-01</span></span></code></pre></div>
<p>This is due to the fact that the <code>hessian.default()</code>
method calls <code>genD()</code>, but the latter never checks if
<code>method.args$show.details</code> is <code>TRUE</code> in the loops
where the extrapolation is carried out. This silent operation mode was
probably implemented to avoid output verbosity since there are many
elements in matrices. Nevertheless, any user who has not looked at the
source code of <code>hessian()</code> would be puzzled by this
unexpected behaviour because the manual of <code>?hessian</code>
explicitly refers to <code>?grad</code> and says that
<code>method.args</code> is passed to <code>grad</code>.</p>
<p></p>
</div>
</div>
<div id="references" class="section level1" number="5">
<h1><span class="header-section-number">5</span> References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-fornberg1988generation" class="csl-entry">
Fornberg, Bengt. 1988. <span>“Generation of Finite Difference Formulas
on Arbitrarily Spaced Grids.”</span> <em>Mathematics of Computation</em>
51 (184): 699–706. <a href="https://doi.org/10.1090/S0025-5718-1988-0935077-0">https://doi.org/10.1090/S0025-5718-1988-0935077-0</a>.
</div>
<div id="ref-kostyrka2025what" class="csl-entry">
Kostyrka, Andreï V. 2025. <span>“What Are You Doing, Step Size: Fast
Computation of Accurate Numerical Derivatives with Finite
Precision.”</span> Working paper.
</div>
<div id="ref-lindfield1989microcomputers" class="csl-entry">
Lindfield, G. R., and J. E. T. Penny. 1989. <em>Microcomputers in
Numerical Analysis</em>. Halsted Press.
</div>
<div id="ref-ridders1982accurate" class="csl-entry">
Ridders, C. J. F. 1982. <span>“Accurate Computation of f’(x) and f’(x)
f”(x).”</span> <em>Advances in Engineering Software</em> 4 (2): 75–76.
https://doi.org/<a href="https://doi.org/10.1016/S0141-1195(82)80057-0">https://doi.org/10.1016/S0141-1195(82)80057-0</a>.
</div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
